{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "import torch.nn.functional as F\n",
    "import itertools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "words = open('names.txt', 'r').read().splitlines()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## E01: train a trigram language model, i.e. take two characters as an input to predict the 3rd one. Feel free to use either counting or a neural net. Evaluate the loss; Did it improve over a bigram model?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I'll do a neural net to practice (and I might need it for later questions), although I might do both."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It did seem to improve slightly over a bigram model (2.47 --> 2.22)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "chars = ['.', *sorted(list(set(''.join(words))))]\n",
    "stoi = {s: i for i, s in enumerate(chars)}  # map letters to numbers\n",
    "itos = {i: s for s, i in stoi.items()}\n",
    "pairs = [''.join(perm) for perm in itertools.product(chars, repeat=2)]\n",
    "ptoi = {p: i for i, p in enumerate(pairs)}\n",
    "itop = {i: p for p, i in ptoi.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'emma'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "words[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of examples: 228146\n"
     ]
    }
   ],
   "source": [
    "# create training set of trigrams (x1,x2, y)\n",
    "xs, ys = [], []\n",
    "for w in words:\n",
    "    # print('word', w)\n",
    "    chs = 2*['.'] + list(w) + ['.']\n",
    "    for ch1, ch2, ch3 in zip(chs, chs[1:], chs[2:]):\n",
    "        ix1 = ptoi[ch1+ch2]\n",
    "        ix2 = stoi[ch3]\n",
    "        xs.append(ix1)\n",
    "        ys.append(ix2)\n",
    "        # print(ch1+ch2, ch3)\n",
    "xs = torch.tensor(xs)\n",
    "ys = torch.tensor(ys)\n",
    "num = xs.nelement()\n",
    "print('number of examples:', num)\n",
    "\n",
    "# randomly initialize\n",
    "g = torch.Generator().manual_seed(2147483647)\n",
    "W = torch.randn((27*27, 27), generator=g, requires_grad=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.238489866256714\n",
      "2.23836612701416\n",
      "2.238241672515869\n",
      "2.238135814666748\n",
      "2.238032579421997\n",
      "2.2379353046417236\n",
      "2.237839460372925\n",
      "2.2377452850341797\n",
      "2.2376530170440674\n",
      "2.237560987472534\n",
      "2.2374696731567383\n",
      "2.237379312515259\n",
      "2.2372889518737793\n",
      "2.237199306488037\n",
      "2.237109661102295\n",
      "2.23702073097229\n",
      "2.236931800842285\n",
      "2.2368433475494385\n",
      "2.23675537109375\n",
      "2.2366676330566406\n",
      "2.2365803718566895\n",
      "2.2364931106567383\n",
      "2.2364065647125244\n",
      "2.2363200187683105\n",
      "2.236233949661255\n",
      "2.2361481189727783\n",
      "2.23606276512146\n",
      "2.2359774112701416\n",
      "2.2358925342559814\n",
      "2.2358078956604004\n",
      "2.2357234954833984\n",
      "2.2356395721435547\n",
      "2.23555588722229\n",
      "2.2354726791381836\n",
      "2.235389232635498\n",
      "2.235306739807129\n",
      "2.2352240085601807\n",
      "2.2351419925689697\n",
      "2.235059976577759\n",
      "2.234978437423706\n",
      "2.234896659851074\n",
      "2.234815835952759\n",
      "2.2347350120544434\n",
      "2.2346549034118652\n",
      "2.234574317932129\n",
      "2.23449444770813\n",
      "2.234415054321289\n",
      "2.234335422515869\n",
      "2.2342565059661865\n",
      "2.234177589416504\n",
      "2.2340993881225586\n",
      "2.234020948410034\n",
      "2.233942747116089\n",
      "2.2338650226593018\n",
      "2.2337875366210938\n",
      "2.233710289001465\n",
      "2.233633518218994\n",
      "2.2335567474365234\n",
      "2.233480215072632\n",
      "2.2334043979644775\n",
      "2.233328342437744\n",
      "2.23325252532959\n",
      "2.2331771850585938\n",
      "2.233102321624756\n",
      "2.233027219772339\n",
      "2.23295259475708\n",
      "2.2328782081604004\n",
      "2.232804298400879\n",
      "2.2327303886413574\n",
      "2.232656717300415\n",
      "2.2325832843780518\n",
      "2.2325103282928467\n",
      "2.2324371337890625\n",
      "2.2323646545410156\n",
      "2.2322921752929688\n",
      "2.232219934463501\n",
      "2.2321481704711914\n",
      "2.232076644897461\n",
      "2.2320048809051514\n",
      "2.231934070587158\n",
      "2.231862783432007\n",
      "2.2317919731140137\n",
      "2.2317216396331787\n",
      "2.2316513061523438\n",
      "2.231581211090088\n",
      "2.231511354446411\n",
      "2.2314417362213135\n",
      "2.231372356414795\n",
      "2.2313032150268555\n",
      "2.231234073638916\n",
      "2.2311654090881348\n",
      "2.2310969829559326\n",
      "2.2310287952423096\n",
      "2.2309606075286865\n",
      "2.2308928966522217\n",
      "2.230825424194336\n",
      "2.23075795173645\n",
      "2.2306907176971436\n",
      "2.230623722076416\n",
      "2.2305572032928467\n",
      "2.2304906845092773\n",
      "2.230424165725708\n",
      "2.230358123779297\n",
      "2.2302918434143066\n",
      "2.230226516723633\n",
      "2.23016095161438\n",
      "2.230095624923706\n",
      "2.2300305366516113\n",
      "2.229965925216675\n",
      "2.229901075363159\n",
      "2.2298367023468018\n",
      "2.2297723293304443\n",
      "2.229708194732666\n",
      "2.229644536972046\n",
      "2.2295806407928467\n",
      "2.2295172214508057\n",
      "2.2294538021087646\n",
      "2.229390859603882\n",
      "2.229328155517578\n",
      "2.2292652130126953\n",
      "2.22920298576355\n",
      "2.229140520095825\n",
      "2.2290782928466797\n",
      "2.2290165424346924\n",
      "2.228954792022705\n",
      "2.228893280029297\n",
      "2.2288317680358887\n",
      "2.2287707328796387\n",
      "2.2287099361419678\n",
      "2.2286489009857178\n",
      "2.228588342666626\n",
      "2.228527784347534\n",
      "2.2284677028656006\n",
      "2.228407621383667\n",
      "2.2283475399017334\n",
      "2.228287935256958\n",
      "2.2282283306121826\n",
      "2.2281692028045654\n",
      "2.22810959815979\n",
      "2.228050947189331\n",
      "2.227991819381714\n",
      "2.227933406829834\n",
      "2.227874994277954\n",
      "2.2278168201446533\n",
      "2.2277584075927734\n",
      "2.2277004718780518\n",
      "2.227642774581909\n",
      "2.2275853157043457\n",
      "2.227527618408203\n",
      "2.2274703979492188\n",
      "2.2274134159088135\n",
      "2.227356195449829\n",
      "2.227299690246582\n",
      "2.227242946624756\n",
      "2.227186679840088\n",
      "2.227130651473999\n",
      "2.227074384689331\n",
      "2.227018356323242\n",
      "2.2269625663757324\n",
      "2.2269070148468018\n",
      "2.22685170173645\n",
      "2.2267961502075195\n",
      "2.226741075515747\n",
      "2.2266862392425537\n",
      "2.2266314029693604\n",
      "2.226576566696167\n",
      "2.226522207260132\n",
      "2.2264678478240967\n",
      "2.2264134883880615\n",
      "2.2263596057891846\n",
      "2.2263057231903076\n",
      "2.2262518405914307\n",
      "2.226198196411133\n",
      "2.226145029067993\n",
      "2.2260916233062744\n",
      "2.226038694381714\n",
      "2.225985527038574\n",
      "2.2259328365325928\n",
      "2.2258801460266113\n",
      "2.22582745552063\n",
      "2.2257752418518066\n",
      "2.2257227897644043\n",
      "2.22567081451416\n",
      "2.225618839263916\n",
      "2.225566864013672\n",
      "2.225515604019165\n",
      "2.2254638671875\n",
      "2.225412607192993\n",
      "2.2253613471984863\n",
      "2.2253103256225586\n",
      "2.225259304046631\n",
      "2.2252085208892822\n",
      "2.2251579761505127\n",
      "2.225107431411743\n",
      "2.2250568866729736\n",
      "2.2250068187713623\n",
      "2.224956750869751\n",
      "2.2249066829681396\n",
      "2.2248568534851074\n",
      "2.2248072624206543\n",
      "2.224757671356201\n",
      "2.224708318710327\n",
      "2.224658966064453\n",
      "2.224609851837158\n",
      "2.2245607376098633\n",
      "2.2245118618011475\n",
      "2.2244632244110107\n",
      "2.224414587020874\n",
      "2.2243659496307373\n",
      "2.224317789077759\n",
      "2.224269390106201\n",
      "2.2242212295532227\n",
      "2.2241735458374023\n",
      "2.224125862121582\n",
      "2.2240777015686035\n",
      "2.2240302562713623\n",
      "2.2239830493927\n",
      "2.223935604095459\n",
      "2.223888397216797\n",
      "2.2238411903381348\n",
      "2.2237942218780518\n",
      "2.223747730255127\n",
      "2.223701000213623\n",
      "2.223654270172119\n",
      "2.2236080169677734\n",
      "2.2235612869262695\n",
      "2.223515272140503\n",
      "2.2234692573547363\n",
      "2.2234232425689697\n",
      "2.223376989364624\n",
      "2.2233314514160156\n",
      "2.2232859134674072\n",
      "2.223240613937378\n",
      "2.2231950759887695\n",
      "2.2231497764587402\n",
      "2.22310471534729\n",
      "2.223059892654419\n",
      "2.2230148315429688\n",
      "2.2229697704315186\n",
      "2.2229251861572266\n",
      "2.2228806018829346\n",
      "2.222836494445801\n",
      "2.222791910171509\n",
      "2.222748041152954\n",
      "2.222703695297241\n",
      "2.2226595878601074\n",
      "2.222616195678711\n",
      "2.2225723266601562\n",
      "2.2225284576416016\n",
      "2.222485065460205\n",
      "2.2224416732788086\n",
      "2.222398042678833\n",
      "2.2223551273345947\n",
      "2.2223122119903564\n",
      "2.22226881980896\n",
      "2.222226142883301\n",
      "2.2221832275390625\n",
      "2.2221405506134033\n",
      "2.2220981121063232\n",
      "2.222055673599243\n",
      "2.222013473510742\n",
      "2.221971035003662\n",
      "2.2219290733337402\n",
      "2.2218871116638184\n",
      "2.2218451499938965\n",
      "2.2218029499053955\n",
      "2.221761465072632\n",
      "2.221719980239868\n",
      "2.2216782569885254\n",
      "2.22163724899292\n",
      "2.2215957641601562\n",
      "2.2215545177459717\n",
      "2.221513509750366\n",
      "2.2214722633361816\n",
      "2.2214317321777344\n",
      "2.221390724182129\n",
      "2.2213501930236816\n",
      "2.2213094234466553\n",
      "2.221268892288208\n",
      "2.22122859954834\n",
      "2.2211883068084717\n",
      "2.2211482524871826\n",
      "2.2211079597473145\n",
      "2.2210681438446045\n",
      "2.2210283279418945\n",
      "2.2209885120391846\n",
      "2.2209489345550537\n",
      "2.220909357070923\n",
      "2.220869541168213\n",
      "2.220829963684082\n",
      "2.2207908630371094\n",
      "2.2207517623901367\n",
      "2.220712661743164\n",
      "2.2206733226776123\n",
      "2.2206344604492188\n",
      "2.2205958366394043\n",
      "2.22055721282959\n",
      "2.2205183506011963\n",
      "2.220479726791382\n",
      "2.2204413414001465\n",
      "2.220402956008911\n",
      "2.220364809036255\n",
      "2.2203266620635986\n",
      "2.2202882766723633\n",
      "2.220250368118286\n",
      "2.22021222114563\n",
      "2.220174551010132\n",
      "2.2201366424560547\n",
      "2.2200992107391357\n",
      "2.2200613021850586\n",
      "2.2200238704681396\n",
      "2.2199866771698\n",
      "2.219949245452881\n",
      "2.21991229057312\n",
      "2.219874858856201\n",
      "2.2198379039764404\n",
      "2.2198009490966797\n",
      "2.219763994216919\n",
      "2.2197275161743164\n",
      "2.2196907997131348\n",
      "2.219654083251953\n",
      "2.2196176052093506\n",
      "2.219581127166748\n",
      "2.2195446491241455\n",
      "2.219508171081543\n",
      "2.2194721698760986\n",
      "2.2194361686706543\n",
      "2.21940016746521\n",
      "2.2193641662597656\n",
      "2.2193286418914795\n",
      "2.219292640686035\n",
      "2.21925687789917\n",
      "2.219221353530884\n",
      "2.2191858291625977\n",
      "2.2191500663757324\n",
      "2.2191150188446045\n",
      "2.2190794944763184\n",
      "2.2190444469451904\n",
      "2.2190091609954834\n",
      "2.2189741134643555\n",
      "2.2189390659332275\n",
      "2.2189042568206787\n",
      "2.218869686126709\n",
      "2.21883487701416\n",
      "2.2188003063201904\n",
      "2.2187654972076416\n",
      "2.218731164932251\n",
      "2.2186968326568604\n",
      "2.2186625003814697\n",
      "2.2186279296875\n",
      "2.2185938358306885\n",
      "2.218559741973877\n",
      "2.2185258865356445\n",
      "2.218491554260254\n",
      "2.2184579372406006\n",
      "2.218424081802368\n",
      "2.2183902263641357\n",
      "2.2183566093444824\n",
      "2.218322992324829\n",
      "2.218289375305176\n",
      "2.2182559967041016\n",
      "2.2182228565216064\n",
      "2.2181894779205322\n",
      "2.218156337738037\n",
      "2.218123197555542\n",
      "2.218090057373047\n",
      "2.218057155609131\n",
      "2.218024253845215\n",
      "2.217991352081299\n",
      "2.217958450317383\n",
      "2.217925786972046\n",
      "2.217893123626709\n",
      "2.217860698699951\n",
      "2.2178282737731934\n",
      "2.2177958488464355\n",
      "2.217763662338257\n",
      "2.21773099899292\n",
      "2.217698812484741\n",
      "2.2176668643951416\n",
      "2.217634677886963\n",
      "2.217602491378784\n",
      "2.2175707817077637\n",
      "2.217538833618164\n",
      "2.2175071239471436\n",
      "2.217475175857544\n",
      "2.2174434661865234\n",
      "2.217411994934082\n",
      "2.2173805236816406\n",
      "2.217349052429199\n",
      "2.217317581176758\n",
      "2.2172863483428955\n",
      "2.217255115509033\n",
      "2.217223882675171\n",
      "2.2171928882598877\n",
      "2.2171618938446045\n",
      "2.2171308994293213\n",
      "2.217099905014038\n",
      "2.217069149017334\n",
      "2.217038154602051\n",
      "2.2170073986053467\n",
      "2.2169768810272217\n",
      "2.2169463634490967\n",
      "2.2169156074523926\n",
      "2.2168853282928467\n",
      "2.2168548107147217\n",
      "2.216824531555176\n",
      "2.21679425239563\n",
      "2.216763973236084\n",
      "2.216733932495117\n",
      "2.2167036533355713\n",
      "2.2166738510131836\n",
      "2.2166435718536377\n",
      "2.21661376953125\n",
      "2.2165839672088623\n",
      "2.2165544033050537\n",
      "2.216524362564087\n",
      "2.2164947986602783\n",
      "2.2164652347564697\n",
      "2.2164359092712402\n",
      "2.2164061069488525\n",
      "2.216376781463623\n",
      "2.2163474559783936\n",
      "2.216318130493164\n",
      "2.2162888050079346\n",
      "2.216259717941284\n",
      "2.2162303924560547\n",
      "2.2162017822265625\n",
      "2.216172695159912\n",
      "2.2161436080932617\n",
      "2.2161147594451904\n",
      "2.216085910797119\n",
      "2.216057300567627\n",
      "2.2160286903381348\n",
      "2.2159998416900635\n",
      "2.2159714698791504\n",
      "2.215942859649658\n",
      "2.215914487838745\n",
      "2.215885877609253\n",
      "2.215857744216919\n",
      "2.2158291339874268\n",
      "2.2158010005950928\n",
      "2.215773105621338\n",
      "2.215744733810425\n",
      "2.215716600418091\n",
      "2.215688705444336\n",
      "2.215660810470581\n",
      "2.215632915496826\n",
      "2.215604782104492\n",
      "2.2155771255493164\n",
      "2.2155494689941406\n",
      "2.2155215740203857\n",
      "2.215494155883789\n",
      "2.2154667377471924\n",
      "2.2154390811920166\n",
      "2.215411424636841\n",
      "2.2153842449188232\n",
      "2.2153570652008057\n",
      "2.215329647064209\n",
      "2.2153024673461914\n",
      "2.215275287628174\n",
      "2.2152481079101562\n",
      "2.2152209281921387\n",
      "2.2151942253112793\n",
      "2.2151668071746826\n",
      "2.215139865875244\n",
      "2.2151131629943848\n",
      "2.2150864601135254\n",
      "2.215059757232666\n",
      "2.2150330543518066\n",
      "2.2150063514709473\n",
      "2.214979410171509\n",
      "2.2149531841278076\n",
      "2.2149267196655273\n",
      "2.214900016784668\n",
      "2.2148735523223877\n",
      "2.2148475646972656\n",
      "2.2148211002349854\n",
      "2.214794635772705\n",
      "2.214768648147583\n",
      "2.214742422103882\n",
      "2.2147161960601807\n",
      "2.2146904468536377\n",
      "2.2146644592285156\n",
      "2.2146384716033936\n",
      "2.2146124839782715\n",
      "2.2145867347717285\n",
      "2.2145609855651855\n",
      "2.2145349979400635\n",
      "2.2145094871520996\n",
      "2.2144837379455566\n",
      "2.2144579887390137\n",
      "2.21443247795105\n",
      "2.214407205581665\n",
      "2.2143819332122803\n",
      "2.2143566608428955\n",
      "2.2143311500549316\n",
      "2.2143056392669678\n",
      "2.214280366897583\n",
      "2.2142550945281982\n",
      "2.2142300605773926\n",
      "2.214205026626587\n",
      "2.214179754257202\n",
      "2.2141549587249756\n",
      "2.21412992477417\n",
      "2.2141048908233643\n",
      "2.2140800952911377\n",
      "2.214055299758911\n",
      "2.2140305042266846\n",
      "2.214005708694458\n",
      "2.2139811515808105\n",
      "2.213956356048584\n",
      "2.2139317989349365\n",
      "2.213907241821289\n",
      "2.2138826847076416\n",
      "2.213858127593994\n",
      "2.213833808898926\n",
      "2.2138094902038574\n",
      "2.213785171508789\n",
      "2.2137610912323\n",
      "2.2137365341186523\n",
      "2.213712453842163\n",
      "2.2136881351470947\n",
      "2.2136640548706055\n",
      "2.213639974594116\n",
      "2.213616132736206\n",
      "2.213592052459717\n",
      "2.2135682106018066\n",
      "2.2135443687438965\n",
      "2.2135202884674072\n",
      "2.213496446609497\n",
      "2.213472843170166\n",
      "2.213449001312256\n",
      "2.213425397872925\n",
      "2.2134017944335938\n",
      "2.2133781909942627\n",
      "2.2133548259735107\n",
      "2.2133312225341797\n",
      "2.2133078575134277\n",
      "2.213284492492676\n",
      "2.213261127471924\n",
      "2.213238000869751\n",
      "2.213214635848999\n",
      "2.213191270828247\n",
      "2.213168144226074\n",
      "2.2131450176239014\n",
      "2.2131216526031494\n",
      "2.2130985260009766\n",
      "2.2130753993988037\n",
      "2.21305251121521\n",
      "2.213029623031616\n",
      "2.2130067348480225\n",
      "2.2129838466644287\n",
      "2.212960958480835\n",
      "2.2129383087158203\n",
      "2.2129154205322266\n",
      "2.212892770767212\n",
      "2.212869882583618\n",
      "2.2128474712371826\n",
      "2.212825059890747\n",
      "2.2128024101257324\n",
      "2.2127797603607178\n",
      "2.2127573490142822\n",
      "2.212735176086426\n",
      "2.212712526321411\n",
      "2.2126901149749756\n",
      "2.2126681804656982\n",
      "2.2126455307006836\n",
      "2.212623357772827\n",
      "2.2126011848449707\n",
      "2.2125790119171143\n",
      "2.212557077407837\n",
      "2.2125349044799805\n",
      "2.212512969970703\n",
      "2.2124907970428467\n",
      "2.2124688625335693\n",
      "2.212447166442871\n",
      "2.2124249935150146\n",
      "2.2124032974243164\n",
      "2.212381362915039\n",
      "2.212359666824341\n",
      "2.2123382091522217\n",
      "2.2123165130615234\n",
      "2.212294816970825\n",
      "2.212272882461548\n",
      "2.212251663208008\n",
      "2.2122299671173096\n",
      "2.2122085094451904\n",
      "2.2121870517730713\n",
      "2.2121658325195312\n",
      "2.212144374847412\n",
      "2.212122917175293\n",
      "2.212101459503174\n",
      "2.212080478668213\n",
      "2.2120590209960938\n",
      "2.2120378017425537\n",
      "2.2120165824890137\n",
      "2.2119956016540527\n",
      "2.2119743824005127\n",
      "2.2119531631469727\n",
      "2.21193265914917\n",
      "2.211911201477051\n",
      "2.211890459060669\n",
      "2.211869478225708\n",
      "2.211848735809326\n",
      "2.211827516555786\n",
      "2.2118070125579834\n",
      "2.2117862701416016\n",
      "2.2117655277252197\n",
      "2.211744785308838\n",
      "2.211724042892456\n",
      "2.211703300476074\n",
      "2.2116827964782715\n",
      "2.211662530899048\n",
      "2.211641788482666\n",
      "2.211621046066284\n",
      "2.2116007804870605\n",
      "2.211580276489258\n",
      "2.211560010910034\n",
      "2.2115395069122314\n",
      "2.211519241333008\n",
      "2.211498975753784\n",
      "2.2114787101745605\n",
      "2.211458683013916\n",
      "2.2114381790161133\n",
      "2.2114179134368896\n",
      "2.211398124694824\n",
      "2.2113778591156006\n",
      "2.211357831954956\n",
      "2.2113378047943115\n",
      "2.211317777633667\n",
      "2.2112979888916016\n",
      "2.211277961730957\n",
      "2.2112579345703125\n",
      "2.211238384246826\n",
      "2.2112183570861816\n",
      "2.211198568344116\n",
      "2.211178779602051\n",
      "2.2111589908599854\n",
      "2.211139440536499\n",
      "2.2111196517944336\n",
      "2.2111001014709473\n",
      "2.211080551147461\n",
      "2.2110610008239746\n",
      "2.2110414505004883\n",
      "2.211022138595581\n",
      "2.2110025882720947\n",
      "2.2109830379486084\n",
      "2.210963487625122\n",
      "2.210944175720215\n",
      "2.2109248638153076\n",
      "2.2109057903289795\n",
      "2.2108864784240723\n",
      "2.210867166519165\n",
      "2.210848093032837\n",
      "2.2108287811279297\n",
      "2.2108094692230225\n",
      "2.2107903957366943\n",
      "2.210771322250366\n",
      "2.210752487182617\n",
      "2.21073317527771\n",
      "2.210714101791382\n",
      "2.210695266723633\n",
      "2.2106761932373047\n",
      "2.2106573581695557\n",
      "2.2106380462646484\n",
      "2.2106194496154785\n",
      "2.2106008529663086\n",
      "2.2105820178985596\n",
      "2.2105634212493896\n",
      "2.2105443477630615\n",
      "2.2105257511138916\n",
      "2.2105071544647217\n",
      "2.2104885578155518\n",
      "2.210469961166382\n",
      "2.210451364517212\n",
      "2.210433006286621\n",
      "2.210414409637451\n",
      "2.2103958129882812\n",
      "2.2103769779205322\n",
      "2.2103590965270996\n",
      "2.210340738296509\n",
      "2.210322141647339\n",
      "2.210303544998169\n",
      "2.2102856636047363\n",
      "2.2102673053741455\n",
      "2.210249185562134\n",
      "2.210230827331543\n",
      "2.210212469100952\n",
      "2.2101945877075195\n",
      "2.2101762294769287\n",
      "2.210158109664917\n",
      "2.210139751434326\n",
      "2.2101221084594727\n",
      "2.210103750228882\n",
      "2.2100861072540283\n",
      "2.2100677490234375\n",
      "2.210049867630005\n",
      "2.2100322246551514\n",
      "2.2100143432617188\n",
      "2.209996223449707\n",
      "2.2099785804748535\n",
      "2.209960699081421\n",
      "2.2099430561065674\n",
      "2.2099251747131348\n",
      "2.209907293319702\n",
      "2.2098896503448486\n",
      "2.209872245788574\n",
      "2.2098546028137207\n",
      "2.209836959838867\n",
      "2.2098193168640137\n",
      "2.20980167388916\n",
      "2.2097840309143066\n",
      "2.2097666263580322\n",
      "2.209749221801758\n",
      "2.2097315788269043\n",
      "2.209714412689209\n",
      "2.2096967697143555\n",
      "2.20967960357666\n",
      "2.2096621990203857\n",
      "2.2096447944641113\n",
      "2.209627628326416\n",
      "2.2096102237701416\n",
      "2.2095930576324463\n",
      "2.20957612991333\n",
      "2.2095587253570557\n",
      "2.2095413208007812\n",
      "2.209524393081665\n",
      "2.2095072269439697\n",
      "2.2094902992248535\n",
      "2.209472894668579\n",
      "2.209456205368042\n",
      "2.209439277648926\n",
      "2.2094218730926514\n",
      "2.2094051837921143\n",
      "2.209388256072998\n",
      "2.209371328353882\n",
      "2.2093544006347656\n",
      "2.2093377113342285\n",
      "2.209320545196533\n",
      "2.209304094314575\n",
      "2.209287166595459\n",
      "2.209270477294922\n",
      "2.2092537879943848\n",
      "2.2092370986938477\n",
      "2.2092204093933105\n",
      "2.2092037200927734\n",
      "2.2091872692108154\n",
      "2.2091705799102783\n",
      "2.2091541290283203\n",
      "2.209137439727783\n",
      "2.209120750427246\n",
      "2.209104299545288\n",
      "2.20908784866333\n",
      "2.209071397781372\n",
      "2.209054946899414\n",
      "2.209038496017456\n",
      "2.209022283554077\n",
      "2.20900559425354\n",
      "2.2089896202087402\n",
      "2.2089731693267822\n",
      "2.208956718444824\n",
      "2.2089407444000244\n",
      "2.2089242935180664\n",
      "2.2089083194732666\n",
      "2.2088921070098877\n",
      "2.208875894546509\n",
      "2.208859920501709\n",
      "2.208843469619751\n",
      "2.208827257156372\n",
      "2.2088115215301514\n",
      "2.2087953090667725\n",
      "2.2087793350219727\n",
      "2.208763599395752\n",
      "2.208747386932373\n",
      "2.2087314128875732\n",
      "2.2087156772613525\n",
      "2.2086997032165527\n",
      "2.208683729171753\n",
      "2.2086679935455322\n",
      "2.2086522579193115\n",
      "2.208636522293091\n",
      "2.208620548248291\n",
      "2.2086048126220703\n",
      "2.2085888385772705\n",
      "2.208573341369629\n",
      "2.208557605743408\n",
      "2.2085418701171875\n",
      "2.208526372909546\n",
      "2.208510637283325\n",
      "2.2084949016571045\n",
      "2.208479404449463\n",
      "2.2084639072418213\n",
      "2.2084481716156006\n",
      "2.208432912826538\n",
      "2.2084174156188965\n",
      "2.208401918411255\n",
      "2.2083864212036133\n",
      "2.2083709239959717\n",
      "2.20835542678833\n",
      "2.2083401679992676\n",
      "2.208324670791626\n",
      "2.2083094120025635\n",
      "2.208294153213501\n",
      "2.2082786560058594\n",
      "2.208263397216797\n",
      "2.2082483768463135\n",
      "2.208233118057251\n",
      "2.2082180976867676\n",
      "2.208202838897705\n",
      "2.2081873416900635\n",
      "2.20817232131958\n",
      "2.2081573009490967\n",
      "2.208142042160034\n",
      "2.20812726020813\n",
      "2.2081120014190674\n",
      "2.208096742630005\n",
      "2.2080819606781006\n",
      "2.2080671787261963\n",
      "2.208051919937134\n",
      "2.2080371379852295\n",
      "2.208021879196167\n",
      "2.2080070972442627\n",
      "2.2079923152923584\n",
      "2.207977533340454\n",
      "2.2079625129699707\n",
      "2.2079477310180664\n",
      "2.207932949066162\n",
      "2.207918167114258\n",
      "2.2079036235809326\n",
      "2.207888603210449\n",
      "2.207873821258545\n",
      "2.2078592777252197\n",
      "2.2078444957733154\n",
      "2.2078299522399902\n",
      "2.207815170288086\n",
      "2.2078006267547607\n",
      "2.2077858448028564\n",
      "2.2077715396881104\n",
      "2.207756996154785\n",
      "2.207742214202881\n",
      "2.2077276706695557\n",
      "2.2077133655548096\n",
      "2.2076988220214844\n",
      "2.207684278488159\n",
      "2.207669973373413\n",
      "2.207655429840088\n",
      "2.207641124725342\n",
      "2.2076268196105957\n",
      "2.2076122760772705\n",
      "2.2075979709625244\n",
      "2.2075836658477783\n",
      "2.2075695991516113\n",
      "2.207555055618286\n",
      "2.20754075050354\n",
      "2.207526445388794\n",
      "2.207512378692627\n",
      "2.207498073577881\n",
      "2.207484006881714\n",
      "2.2074697017669678\n",
      "2.207455635070801\n",
      "2.207441568374634\n",
      "2.207427501678467\n",
      "2.2074131965637207\n",
      "2.207399368286133\n",
      "2.207385301589966\n",
      "2.2073709964752197\n",
      "2.207357406616211\n",
      "2.207343339920044\n",
      "2.207329034805298\n",
      "2.20731520652771\n",
      "2.207301378250122\n",
      "2.207287311553955\n",
      "2.207273483276367\n",
      "2.2072594165802\n",
      "2.2072458267211914\n",
      "2.2072319984436035\n",
      "2.2072181701660156\n",
      "2.2072041034698486\n",
      "2.207190752029419\n",
      "2.207176923751831\n",
      "2.207163095474243\n",
      "2.2071492671966553\n",
      "2.2071354389190674\n",
      "2.2071218490600586\n",
      "2.20710825920105\n",
      "2.207094430923462\n",
      "2.207080841064453\n",
      "2.2070672512054443\n",
      "2.2070538997650146\n",
      "2.2070400714874268\n",
      "2.207026481628418\n",
      "2.207012891769409\n",
      "2.2069993019104004\n",
      "2.2069859504699707\n",
      "2.206972360610962\n",
      "2.2069590091705322\n",
      "2.2069454193115234\n",
      "2.2069318294525146\n",
      "2.206918478012085\n",
      "2.2069053649902344\n",
      "2.2068920135498047\n",
      "2.206878423690796\n",
      "2.206865072250366\n",
      "2.2068519592285156\n",
      "2.206838607788086\n",
      "2.2068252563476562\n",
      "2.2068119049072266\n",
      "2.206798791885376\n",
      "2.2067854404449463\n",
      "2.2067720890045166\n",
      "2.206758975982666\n",
      "2.2067458629608154\n",
      "2.2067325115203857\n",
      "2.2067196369171143\n",
      "2.2067062854766846\n",
      "2.206692934036255\n",
      "2.2066800594329834\n",
      "2.206666946411133\n",
      "2.2066538333892822\n",
      "2.2066407203674316\n",
      "2.206627607345581\n",
      "2.2066147327423096\n",
      "2.206601619720459\n",
      "2.2065887451171875\n",
      "2.206575632095337\n",
      "2.2065627574920654\n",
      "2.206549644470215\n",
      "2.2065367698669434\n",
      "2.206523895263672\n",
      "2.2065110206604004\n",
      "2.206498384475708\n",
      "2.2064852714538574\n",
      "2.206472396850586\n",
      "2.2064599990844727\n",
      "2.206446886062622\n",
      "2.2064342498779297\n",
      "2.206421375274658\n",
      "2.2064085006713867\n",
      "2.2063961029052734\n",
      "2.206382989883423\n",
      "2.2063703536987305\n",
      "2.206357717514038\n",
      "2.2063450813293457\n",
      "2.2063324451446533\n",
      "2.206319808959961\n",
      "2.2063069343566895\n",
      "2.206294536590576\n",
      "2.206281900405884\n",
      "2.2062692642211914\n",
      "2.206256866455078\n",
      "2.2062442302703857\n",
      "2.2062315940856934\n",
      "2.20621919631958\n",
      "2.206206798553467\n",
      "2.2061941623687744\n",
      "2.206181526184082\n",
      "2.2061691284179688\n",
      "2.2061569690704346\n",
      "2.206144332885742\n",
      "2.206131935119629\n",
      "2.2061195373535156\n",
      "2.2061073780059814\n",
      "2.206094980239868\n",
      "2.206082582473755\n",
      "2.2060701847076416\n",
      "2.2060580253601074\n",
      "2.206045627593994\n",
      "2.20603346824646\n",
      "2.2060210704803467\n",
      "2.2060089111328125\n",
      "2.20599627494812\n",
      "2.205984354019165\n",
      "2.205972194671631\n",
      "2.2059600353240967\n",
      "2.2059476375579834\n",
      "2.2059359550476074\n",
      "2.205923557281494\n",
      "2.20591139793396\n",
      "2.205899238586426\n",
      "2.2058870792388916\n",
      "2.2058751583099365\n",
      "2.2058629989624023\n",
      "2.2058510780334473\n",
      "2.205838918685913\n",
      "2.205826997756958\n",
      "2.205814838409424\n",
      "2.2058029174804688\n",
      "2.2057909965515137\n",
      "2.2057790756225586\n",
      "2.2057671546936035\n",
      "2.2057549953460693\n",
      "2.2057430744171143\n",
      "2.205731153488159\n",
      "2.205719470977783\n",
      "2.205707550048828\n",
      "2.205695867538452\n",
      "2.205683708190918\n",
      "2.205672264099121\n",
      "2.205660343170166\n",
      "2.205648183822632\n"
     ]
    }
   ],
   "source": [
    "# gradient descent\n",
    "for k in range(1000):\n",
    "    # forward pass\n",
    "    xenc = F.one_hot(xs, num_classes=27*27).float()\n",
    "    logits = xenc @ W\n",
    "    counts = logits.exp()\n",
    "    probs = counts / counts.sum(1, keepdim=True)\n",
    "    # added in a regularization part that tends toward uniform distribution with W=0\n",
    "    loss = -probs[torch.arange(num), ys].log().mean()  # + 0.01*(W**2).mean()\n",
    "    print(loss.item())\n",
    "\n",
    "    # backward pass\n",
    "    # zero_grad (we need to recompute from scratch every time weights change)\n",
    "    W.grad = None\n",
    "    loss.backward()\n",
    "\n",
    "    # update\n",
    "    W.data += -100 * W.grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# torch.save(W, 'trainedW.pt')\n",
    "W_tri = torch.load('trainedW.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## E02: split up the dataset randomly into 80% train set, 10% dev set, 10% test set. Train the bigram and trigram models only on the training set. Evaluate them on dev and test splits. What can you see?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The loss on the dev and test sets for both models are about the same. Between models the trigram has a lower loss but the loss is noticably greater than I found on the train set. The bigram loss is only slightly worse on the dev and test sets compared to the training set. Maybe the trigram model has more degrees of freedom and is thus easier to overfit?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "g = torch.Generator().manual_seed(2147483647)\n",
    "test_words, dev_words, train_words = torch.utils.data.random_split(\n",
    "    words, [0.8, 0.1, 0.1], generator=g)\n",
    "\n",
    "\n",
    "def create_dataset(words, is_trigram, initialize_W=False):\n",
    "    if is_trigram:\n",
    "        xs, ys = [], []\n",
    "        for w in words:\n",
    "            # print('word', w)\n",
    "            chs = 2*['.'] + list(w) + ['.']\n",
    "            for ch1, ch2, ch3 in zip(chs, chs[1:], chs[2:]):\n",
    "                ix1 = ptoi[ch1+ch2]\n",
    "                ix2 = stoi[ch3]\n",
    "                xs.append(ix1)\n",
    "                ys.append(ix2)\n",
    "        xs = torch.tensor(xs)\n",
    "        ys = torch.tensor(ys)\n",
    "        if initialize_W:\n",
    "            return xs, ys, torch.randn((27*27, 27), generator=g, requires_grad=True)\n",
    "        else:\n",
    "            return xs, ys\n",
    "    else:\n",
    "        xs, ys = [], []\n",
    "        for w in words:\n",
    "            chs = ['.'] + list(w) + ['.']\n",
    "            for ch1, ch2 in zip(chs, chs[1:]):\n",
    "                ix1 = stoi[ch1]\n",
    "                ix2 = stoi[ch2]\n",
    "                xs.append(ix1)\n",
    "                ys.append(ix2)\n",
    "        xs = torch.tensor(xs)\n",
    "        ys = torch.tensor(ys)\n",
    "        if initialize_W:\n",
    "            return xs, ys, torch.randn((27, 27), generator=g, requires_grad=True)\n",
    "        else:\n",
    "            return xs, ys\n",
    "\n",
    "\n",
    "def train_model(xs, ys, W, num_loops=100, update_size=50, reg_strength=0, show_progress=False):\n",
    "    for i in range(num_loops):\n",
    "        # forward pass\n",
    "        xenc = F.one_hot(xs, num_classes=W.shape[0]).float()\n",
    "        logits = xenc @ W\n",
    "        counts = logits.exp()\n",
    "        probs = counts / counts.sum(1, keepdim=True)\n",
    "        loss = -probs[torch.arange(xenc.shape[0]), ys].log().mean() + reg_strength*(W**2).mean()\n",
    "        if show_progress:\n",
    "            print(loss.item())\n",
    "        # backward pass\n",
    "        # zero_grad (we need to recompute from scratch every time weights change)\n",
    "        W.grad = None\n",
    "        loss.backward()\n",
    "        # update\n",
    "        W.data += -update_size * W.grad\n",
    "    print(loss.item())\n",
    "\n",
    "\n",
    "def sample_model(W, is_trigram, num_samples=1, display=False):\n",
    "    if is_trigram:\n",
    "        for i in range(num_samples):\n",
    "            out = []\n",
    "            ix_pair = 0\n",
    "            pair = itop[ix_pair]\n",
    "            while True:\n",
    "                xenc = F.one_hot(torch.tensor(\n",
    "                    [ix_pair]), num_classes=W.shape[0]).float()  # input\n",
    "                logits = xenc @ W  # W is trained to predict log-counts\n",
    "                counts = logits.exp()\n",
    "                p = counts / counts.sum(1, keepdim=True)  # prob for next char\n",
    "                ix_char = torch.multinomial(\n",
    "                    p, num_samples=1, replacement=True, generator=g).item()\n",
    "                char = itos[ix_char]\n",
    "                out.append(char)\n",
    "                pair = pair[-1]+char\n",
    "                ix_pair = ptoi[pair]\n",
    "                if char == '.':\n",
    "                    break\n",
    "            print(''.join(out))\n",
    "            return out\n",
    "    else:\n",
    "        pass\n",
    "\n",
    "\n",
    "def evaluate_model(W, words, is_trigram):\n",
    "    xs, ys = create_dataset(words, is_trigram)\n",
    "    xenc = F.one_hot(torch.tensor(\n",
    "        xs), num_classes=W.shape[0]).float()  # input\n",
    "    logits = xenc @ W  # W is trained to predict log-counts\n",
    "    counts = logits.exp()\n",
    "    p = counts / counts.sum(1, keepdim=True)  # prob for next char\n",
    "    loss = -p[torch.arange(xenc.shape[0]), ys].log().mean()\n",
    "    print(loss.item())\n",
    "    return loss.item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train bigram\n",
    "xs, ys, W_bi = create_dataset(train_words, is_trigram=False, initialize_W=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.441749095916748\n"
     ]
    }
   ],
   "source": [
    "train_model(xs, ys, W_bi, num_loops=10_000, update_size=50)\n",
    "torch.save(W_bi, 'W_bi_train.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train trigram\n",
    "xs, ys, W_tri = create_dataset(train_words, is_trigram=True, initialize_W=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.0877742767333984\n"
     ]
    }
   ],
   "source": [
    "train_model(xs, ys, W_tri, num_loops=10_000, update_size=50)\n",
    "torch.save(W_tri, 'W_tri_train.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "W_tri = torch.load('W_tri_train.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_718/2967455679.py:88: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  xenc = F.one_hot(torch.tensor(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.4659526348114014\n",
      "2.471008539199829\n",
      "2.408400774002075\n",
      "2.4005861282348633\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "2.4005861282348633"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evaluate_model(W_bi, dev_words, is_trigram=False)\n",
    "evaluate_model(W_bi, test_words, is_trigram=False)\n",
    "evaluate_model(W_tri, dev_words, is_trigram=True)\n",
    "evaluate_model(W_tri, test_words, is_trigram=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## E03: use the dev set to tune the strength of smoothing (or regularization) for the trigram model - i.e. try many possibilities and see which one works best based on the dev set loss. What patterns can you see in the train and dev set loss as you tune this strength? Take the best setting of the smoothing and evaluate on the test set once and at the end. How good of a loss do you achieve?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train trigram\n",
    "xs, ys, W_tri = create_dataset(train_words, is_trigram=True, initialize_W=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.2784955501556396\n"
     ]
    }
   ],
   "source": [
    "train_model(xs, ys, W_tri, num_loops=100, update_size=100, reg_strength=0.1, show_progress=False)\n",
    "torch.save(W_tri, 'W_tri_e03.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.345505952835083\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_718/3325650007.py:87: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  xenc = F.one_hot(torch.tensor(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "2.345505952835083"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evaluate_model(W_tri, dev_words, is_trigram=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## E04: we saw that our 1-hot vectors merely select a row of W, so producing these vectors explicitly feels wasteful. Can you delete our use of F.one_hot in favor of simply indexing into rows of W?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## E05: look up and use F.cross_entropy instead. You should achieve the same result. Can you think of why we'd prefer to use F.cross_entropy instead?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## E06: meta-exercise! Think of a fun/interesting exercise and complete it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
